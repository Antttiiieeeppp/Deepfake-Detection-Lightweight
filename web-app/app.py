import streamlit as st
import pandas as pd
import numpy as np
import time
import sys
# hw
import cv2
import tempfile
import os
from PIL import Image
import numpy as np
import mediapipe as mp
import torch
import torch.nn.functional as F
import yaml
# jh
from torchvision import transforms
# ëª¨ë¸ class ì •ì˜
import sys
import os

# ìƒëŒ€ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../efficient-vit")))
# EfficientViT í´ë˜ìŠ¤ ì„í¬íŠ¸
from efficient_vit import EfficientViT
from cnn import SimpleCNN

# from ..efficient_vit import EfficientViT
# sa
from modify_video import add_badge_with_timestamps
BADGE_PATH = "Faces Resource.png"

# efficient_vit_path = "../efficient-vit/efficient_vit.py"
# if efficient_vit_path not in sys.path:
#     sys.path.append(efficient_vit_path)

# functions
# í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì— ë„£ê¸° ìœ„í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_image(cropped_face):
    # OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
    cropped_face = Image.fromarray(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB))
    cropped_face = cropped_face.resize((224, 224))  # ëª¨ë¸ì— ë§ëŠ” í¬ê¸°
    cropped_face = np.array(cropped_face).transpose((2, 0, 1))  # (H, W, C) -> (C, H, W)
    cropped_face = torch.tensor(cropped_face, dtype=torch.float32) / 255.0  # 0~1ë¡œ ì •ê·œí™”
    cropped_face = cropped_face.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    return cropped_face

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict(cropped_face):
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    input_tensor = preprocess_image(cropped_face)
    
    # ëª¨ë¸ì— ì…ë ¥
    with torch.no_grad():
        output = model(input_tensor)  # ëª¨ë¸ì˜ ì¶œë ¥
        # ì¶œë ¥ì´ í™•ë¥ ì¼ ê²½ìš° (sigmoidë¥¼ í†µê³¼ì‹œì¼œì„œ 0~1ë¡œ ë§Œë“¤ê¸°)
        prob = torch.sigmoid(output)
        
        # í™•ë¥ ì„ 0.5 ê¸°ì¤€ìœ¼ë¡œ 1 ë˜ëŠ” 0ìœ¼ë¡œ ë³€í™˜
        prediction = 1 if prob > 0.5 else 0
    return prediction, prob.item()

# 2FPSë¡œ í”„ë ˆì„ ì¶”ì¶œ
# def extract_frames(video_path, fps=2):    # ë™ì¼í•œ í•¨ìˆ˜ 2ê°œ -> í•˜ë‚˜ëŠ” ì£¼ì„ ì²˜ë¦¬ í•˜ê² ìŠµë‹ˆë‹¤
#     cap = cv2.VideoCapture(video_path)
#     original_fps = int(cap.get(cv2.CAP_PROP_FPS))
#     frame_interval = original_fps // fps
#     frames = []
#     count = 0

#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break
#         if count % frame_interval == 0:
#             frames.append(frame)
#         count += 1

#     cap.release()
#     return frames

# # ì–¼êµ´ ê°ì§€ ë° í¬ë¡­
# def detect_and_crop_faces(frames):
#     cropped_faces = []
#     mp_face_detection = mp.solutions.face_detection
#     face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.7)  # min_detection_confidence ì¡°ì •

#     for frame in frames:
#         # ì–¼êµ´ ê°ì§€
#         results = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#         if results.detections:
#             for detection in results.detections:
#                 bboxC = detection.location_data.relative_bounding_box
#                 h, w, _ = frame.shape
#                 x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)
                
#                 # ì–¼êµ´ í¬ë¡­
#                 cropped_face = frame[y:y + h_box, x:x + w_box]
                
#                 # 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
#                 resized_face = cv2.resize(cropped_face, (224, 224))
#                 cropped_faces.append(resized_face)
#         else:
#             cropped_faces.append(None)
    
#     return cropped_faces

# Helper functions for video processing
def extract_frames(video_path, fps=2):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
    cap.release()

    
    # FPSì— ë§ì¶° í”„ë ˆì„ì„ ì¶”ì¶œ
    total_frames = len(frames)
    interval = int(total_frames / (fps * (total_frames / 30)))  # FPSì— ë§ëŠ” ê°„ê²© ê³„ì‚°
    selected_frames = frames[::interval]  # 2FPSë¡œ í”„ë ˆì„ ì„ íƒ

    return selected_frames

def detect_and_crop_faces(frames):
    cropped_faces = []
    mp_face_detection = mp.solutions.face_detection
    face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)
    
    for frame in frames:
        # ì–¼êµ´ ê°ì§€
        results = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                h, w, _ = frame.shape
                x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)
                
                # ì–¼êµ´ í¬ë¡­
                cropped_face = frame[y:y + h_box, x:x + w_box]
                
                # 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                resized_face = cv2.resize(cropped_face, (224, 224))
                cropped_faces.append(resized_face)
    return cropped_faces
###########################
# jh

# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜

def load_model(model_path: str, device):
    # model = EfficientViT(config=config, channels=1280, selected_efficient_net = 0)
    model = SimpleCNN()
    
    # CPUì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ (GPU ì‚¬ìš© ì‹œ map_location='cuda' ë“± ë³€ê²½)
    # model.load_state_dict(torch.load(model_path, map_location='cpu'))
    model.load_state_dict(torch.load(model_path))
    model = model.to(device)
    model.eval()
    return model



# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (face_np -> í…ì„œ)
def preprocess_image(face_np: np.ndarray):
    """
    - face_np: (H, W, 3) NumPy, OpenCV(BGR) í˜•ì‹ì˜ ì–¼êµ´ ì´ë¯¸ì§€.
    - (1) OpenCV -> PIL(RGB) ë³€í™˜
    - (2) ToTensor + Resize ë“±ìœ¼ë¡œ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
    """
    transform = transforms.Compose([
        transforms.ToTensor(),             
        transforms.Resize((224, 224)),     
    ])
    
    # OpenCV(BGR) -> PIL(RGB)
    face_pil = Image.fromarray(cv2.cvtColor(face_np, cv2.COLOR_BGR2RGB))
    tensor = transform(face_pil).unsqueeze(0)  #
    return tensor

def detect_deepfake(model, cropped_faces_dict):
    results = {}

    for idx, face_np in cropped_faces_dict.items():
        if face_np is None:
            # ì–¼êµ´ì´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ ë¶ˆê°€ -> None ì²˜ë¦¬
            results[idx] = None
            continue

        # ì „ì²˜ë¦¬ (preprocess_image í•¨ìˆ˜ ì‚¬ìš©)
        input_tensor = preprocess_image(face_np)  # (1, C, H, W) í…ì„œ
        input_tensor = input_tensor.to(device)


        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            output = model(input_tensor)         # shape: (1,1) ê°€ì •
            prob = torch.sigmoid(output).item()  # 0~1 í™•ë¥ 
            prediction = 1 if prob > 0.7 else 0   # threshold=0.5

        # ê²°ê³¼: {ì¸ë±ìŠ¤: ì˜ˆì¸¡ê°’} í˜•íƒœë¡œ ì €ì¥
        results[idx] = prediction

    return results
##########################
# real .yaml file
def load_yaml_config(file_path):
    with open(file_path, 'r') as file:
        config = yaml.safe_load(file)
    return config


device = "cuda" if torch.cuda.is_available() else "cpu"
# Call your detection model here
CONFIG_PATH = "../efficient-vit/configs/architecture.yaml"  # Replace with the actual path
config = load_yaml_config(CONFIG_PATH)
# ëª¨ë¸ ë¡œë“œ
# MODEL_PATH = "../efficient-vit/pretrained_models/efficient_vit.pth"
MODEL_PATH = "../efficient-vit/pretrained_models/kd_cnn.pth"
model = load_model(MODEL_PATH, device)
###############################

# Page configuration
st.set_page_config(
    page_title="Anttiep's Dashboard",
    page_icon="ğŸ‚",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Sidebar for navigation
with st.sidebar:
    st.title("Page Overview")
    page = st.radio("Go to a page:", ["Home", "Upload Video", "About"])

# 1. Home Page
if page == "Home":
    st.title("ğŸ­ Team ANTTTIIIEEEPPP's Demo")
    
    # Project Introduction
    st.header("Project ì†Œê°œ :sunglasses:", divider=True)
    multi = '''
    :rainbow[ë”¥í˜ì´í¬ íƒì§€ ëª¨ë¸]ì€ ì´ë¯¸ ë†’ì€ ì„±ëŠ¥ì„ ê°–ì¶”ê³  ìˆìœ¼ë‚˜, ì—°ì‚° ìì›ì´ ì œí•œëœ í™˜ê²½ì—ì„œëŠ” í™œìš©ì´ ì œí•œì ì…ë‹ˆë‹¤.
    ê·¸ë ‡ê¸°ì— ê²½ëŸ‰í™”ëœ ë”¥í˜ì´í¬ íƒì§€ ëª¨ë¸ë¡œ ì´ëŸ¬í•œ ê¸°ìˆ ì  ì¥ë²½ì„ ë‚®ì¶°,
    ë‹¤ì–‘í•œ ê¸°ì—… ì‹œìŠ¤í…œ ë° ê¸°ì¡´ SNS í”Œë«í¼ì— ë”¥í˜ì´í¬ íƒì§€ ê¸°ëŠ¥ì„ ì†ì‰½ê²Œ í†µí•©í•˜ê³ ì í•©ë‹ˆë‹¤.
    ë”¥í˜ì´í¬ í™•ì‚° ë°©ì§€ë¥¼ ìœ„í•œ ì‹¤ì‹œê°„ ëŒ€ì‘ì´ ê°€ëŠ¥í•œ ê²½ëŸ‰í™” AI ëª¨ë¸ ê°œë°œì„ ëª©í‘œë¡œ ë‘ê³  ìˆìŠµë‹ˆë‹¤.
    '''
    st.markdown(multi)
    
    # Example Video Section
    st.header("í”„ë¡œì íŠ¸ ë°°í¬ ì˜ˆì‹œ", divider=True)
    VIDEO_URL = "https://www.youtube.com/watch?v=HhMPSuZgJsc"
    st.video(VIDEO_URL)


# 2. Video Upload Page
elif page == "Upload Video":
    st.title("Detect Deepfakes in Videos")
    uploaded_video = st.file_uploader("Upload a Video", type=["mp4"])   # ì¶”ê°€ í• ê²ƒ : avi, mov
    
    if uploaded_video is not None:
        st.video(uploaded_video)
        st.markdown("**Analyzing video for deepfakes...**")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video:
            temp_video.write(uploaded_video.read())
            video_path = temp_video.name
            # output_video_path = temp_video.name ####
        
        # 2FPSë¡œ í”„ë ˆì„ ì¶”ì¶œ

        frames = extract_frames(video_path, fps=2)
        total_frames = len(frames)
        st.write(f"Extracted {total_frames} frames from the video.")
        
        # ì–¼êµ´ ê°ì§€ ë° í¬ë¡­
        cropped_faces = detect_and_crop_faces(frames)
        total_cropped_faces = len(cropped_faces)
        st.write(f"Detected and cropped {total_cropped_faces} faces.")
        # st.write(cropped_faces)
        # # í¬ë¡­ëœ ì–¼êµ´ ì¶œë ¥
        # if cropped_faces:
        #     st.write("Cropped Faces:")
        #     for i, face in enumerate(cropped_faces):
        #         face_image = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
        #         st.image(face_image, caption=f"Cropped Face {i+1}", width=150)  # widthë¥¼ 150ìœ¼ë¡œ ì„¤ì •
        # else:
        #     st.write("No faces detected in the video.")
        
        # í¬ë¡­ëœ ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ê¸°
        cropped_faces_dict = {index: value for index, value in enumerate(cropped_faces)}
        #ë§Œë“  ë”•ì…”í„°ë¦¬ ì¶œë ¥
        #st.write(cropped_faces_dict)

        # # ë”¥í˜ì´í¬ íŒë³„
        predictions_dict = detect_deepfake(model, cropped_faces_dict)
        # í…ŒìŠ¤íŠ¸
        print(predictions_dict)

        if isinstance(predictions_dict, dict):  # Check if predictions_dict is a valid dictionary
            try:
                # Add badge to the video at specific timestamps
                modified_video = add_badge_with_timestamps(video_path, BADGE_PATH, predictions_dict)
                
                # Display the processed video in the Streamlit app
                st.video(modified_video)
                st.success("Deepfake detected!")
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.remove(video_path)
            except Exception as e:
                st.error(f"An error occurred while processing the video: {e}")
        else:
            st.warning("Could not analyze the video. Please try again.")
      
# 3. About Page
elif page == "About":
    st.title("About the Project")
    st.markdown("""
    This project aims to combat the misuse of deepfake technology by providing lightweight AI models 
    capable of real-time detection in resource-limited environments. 
    
    (need to add more here..)
    """)

 # Contributors Section
    st.subheader("Contributors âœ¨", divider="grey")
    
    # Contributor data
    contributors = [
        {
            "name": "HyeongJun Kim",
            "github": "https://github.com/hoooddy",
            "avatar": "https://avatars.githubusercontent.com/u/35017649?v=4",
        },
        {
            "name": "HyeonWoo Choi",
            "github": "https://github.com/po2955",
            "avatar": "https://avatars.githubusercontent.com/u/84663334?v=4",
        },
        {
            "name": "JaeHee Lee",
            "github": "https://github.com/JaeHeeLE",
            "avatar": "https://avatars.githubusercontent.com/u/153152453?v=4",
        },
        {
            "name": "Seunga Kim",
            "github": "https://github.com/sinya3558",
            "avatar": "https://avatars.githubusercontent.com/u/70243358?v=4",
        }
    ]
    
    # Display contributors
    cols = st.columns(len(contributors))
    for col, contributor in zip(cols, contributors):
        with col:
            st.image(contributor["avatar"], width=100)
            st.markdown(f"**[{contributor['name']}]({contributor['github']})**")
            
# GitHub Repo Section
    st.subheader("GitHub Repository ğŸ‘¾", divider=True)
    repo_text = '''
    If you want to check our further updates, 
    
    ğŸ‘‰ **Visit our GitHub Repo:** [ğŸ’» Antttiiieeeppp](https://github.com/Antttiiieeeppp/Deepfake-Detection-Lightweight)
    '''
    st.markdown(repo_text)
    
# Location Section
    st.subheader("Where we are ğŸ¦", divider=True)
    st.caption("Toegye-ro, Jung-go,")
    st.caption("Seoul, Republic of Korea")
    