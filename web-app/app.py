import streamlit as st
import pandas as pd
import numpy as np
import time
import sys
# hw
import cv2
import tempfile
import os
from PIL import Image
import numpy as np
import mediapipe as mp
import torch
import torch.nn.functional as F
import yaml
# jh
from torchvision import transforms
# Î™®Îç∏ class Ï†ïÏùò
import sys
import os

# ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º sys.pathÏóê Ï∂îÍ∞Ä
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../efficient-vit")))
# EfficientViT ÌÅ¥ÎûòÏä§ ÏûÑÌè¨Ìä∏
from efficient_vit import EfficientViT
from cnn import SimpleCNN

# from ..efficient_vit import EfficientViT
# sa
from modify_video import add_badge_with_timestamps
BADGE_PATH = "Faces Resource.png"

# efficient_vit_path = "../efficient-vit/efficient_vit.py"
# if efficient_vit_path not in sys.path:
#     sys.path.append(efficient_vit_path)

# functions
# ÌÅ¨Î°≠Îêú Ïù¥ÎØ∏ÏßÄÎ•º Î™®Îç∏Ïóê ÎÑ£Í∏∞ ÏúÑÌïú Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò
def preprocess_image(cropped_face):
    # OpenCVÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄÎ•º ÌÖêÏÑúÎ°ú Î≥ÄÌôò
    cropped_face = Image.fromarray(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB))
    cropped_face = cropped_face.resize((224, 224))  # Î™®Îç∏Ïóê ÎßûÎäî ÌÅ¨Í∏∞
    cropped_face = np.array(cropped_face).transpose((2, 0, 1))  # (H, W, C) -> (C, H, W)
    cropped_face = torch.tensor(cropped_face, dtype=torch.float32) / 255.0  # 0~1Î°ú Ï†ïÍ∑úÌôî
    cropped_face = cropped_face.unsqueeze(0)  # Î∞∞Ïπò Ï∞®Ïõê Ï∂îÍ∞Ä
    return cropped_face

# ÏòàÏ∏° Ìï®Ïàò
def predict(cropped_face):
    # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
    input_tensor = preprocess_image(cropped_face)
    
    # Î™®Îç∏Ïóê ÏûÖÎ†•
    with torch.no_grad():
        output = model(input_tensor)  # Î™®Îç∏Ïùò Ï∂úÎ†•
        # Ï∂úÎ†•Ïù¥ ÌôïÎ•†Ïùº Í≤ΩÏö∞ (sigmoidÎ•º ÌÜµÍ≥ºÏãúÏºúÏÑú 0~1Î°ú ÎßåÎì§Í∏∞)
        prob = torch.sigmoid(output)
        
        # ÌôïÎ•†ÏùÑ 0.5 Í∏∞Ï§ÄÏúºÎ°ú 1 ÎòêÎäî 0ÏúºÎ°ú Î≥ÄÌôò
        prediction = 1 if prob > 0.5 else 0
    return prediction, prob.item()

# 2FPSÎ°ú ÌîÑÎ†àÏûÑ Ï∂îÏ∂ú
# def extract_frames(video_path, fps=2):    # ÎèôÏùºÌïú Ìï®Ïàò 2Í∞ú -> ÌïòÎÇòÎäî Ï£ºÏÑù Ï≤òÎ¶¨ ÌïòÍ≤†ÏäµÎãàÎã§
#     cap = cv2.VideoCapture(video_path)
#     original_fps = int(cap.get(cv2.CAP_PROP_FPS))
#     frame_interval = original_fps // fps
#     frames = []
#     count = 0

#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break
#         if count % frame_interval == 0:
#             frames.append(frame)
#         count += 1

#     cap.release()
#     return frames

# # ÏñºÍµ¥ Í∞êÏßÄ Î∞è ÌÅ¨Î°≠
# def detect_and_crop_faces(frames):
#     cropped_faces = []
#     mp_face_detection = mp.solutions.face_detection
#     face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.7)  # min_detection_confidence Ï°∞Ï†ï

#     for frame in frames:
#         # ÏñºÍµ¥ Í∞êÏßÄ
#         results = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#         if results.detections:
#             for detection in results.detections:
#                 bboxC = detection.location_data.relative_bounding_box
#                 h, w, _ = frame.shape
#                 x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)
                
#                 # ÏñºÍµ¥ ÌÅ¨Î°≠
#                 cropped_face = frame[y:y + h_box, x:x + w_box]
                
#                 # 224x224Î°ú Î¶¨ÏÇ¨Ïù¥Ï¶à
#                 resized_face = cv2.resize(cropped_face, (224, 224))
#                 cropped_faces.append(resized_face)
#         else:
#             cropped_faces.append(None)
    
#     return cropped_faces

# Helper functions for video processing
def extract_frames(video_path, fps=2):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
    cap.release()

    
    # FPSÏóê ÎßûÏ∂∞ ÌîÑÎ†àÏûÑÏùÑ Ï∂îÏ∂ú
    total_frames = len(frames)
    interval = int(total_frames / (fps * (total_frames / 30)))  # FPSÏóê ÎßûÎäî Í∞ÑÍ≤© Í≥ÑÏÇ∞
    selected_frames = frames[::interval]  # 2FPSÎ°ú ÌîÑÎ†àÏûÑ ÏÑ†ÌÉù

    return selected_frames

def detect_and_crop_faces(frames):
    cropped_faces = []
    mp_face_detection = mp.solutions.face_detection
    face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)
    
    for frame in frames:
        # ÏñºÍµ¥ Í∞êÏßÄ
        results = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                h, w, _ = frame.shape
                x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)
                
                # ÏñºÍµ¥ ÌÅ¨Î°≠
                cropped_face = frame[y:y + h_box, x:x + w_box]
                
                # 224x224Î°ú Î¶¨ÏÇ¨Ïù¥Ï¶à
                resized_face = cv2.resize(cropped_face, (224, 224))
                cropped_faces.append(resized_face)
    return cropped_faces
###########################
# jh

# Î™®Îç∏ Î°úÎìú Ìï®Ïàò

def load_model(model_path: str, device):
    # model = EfficientViT(config=config, channels=1280, selected_efficient_net = 0)
    model = SimpleCNN()
    
    # CPUÏóêÏÑú Í∞ÄÏ§ëÏπò Î°úÎìú (GPU ÏÇ¨Ïö© Ïãú map_location='cuda' Îì± Î≥ÄÍ≤Ω)
    # model.load_state_dict(torch.load(model_path, map_location='cpu'))
    model.load_state_dict(torch.load(model_path))
    model = model.to(device)
    model.eval()
    return model



# Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ (face_np -> ÌÖêÏÑú)
def preprocess_image(face_np: np.ndarray):
    """
    - face_np: (H, W, 3) NumPy, OpenCV(BGR) ÌòïÏãùÏùò ÏñºÍµ¥ Ïù¥ÎØ∏ÏßÄ.
    - (1) OpenCV -> PIL(RGB) Î≥ÄÌôò
    - (2) ToTensor + Resize Îì±ÏúºÎ°ú Î™®Îç∏ ÏûÖÎ†• ÌòïÌÉúÎ°ú Î≥ÄÌôò
    """
    transform = transforms.Compose([
        transforms.ToTensor(),             
        transforms.Resize((224, 224)),     
    ])
    
    # OpenCV(BGR) -> PIL(RGB)
    face_pil = Image.fromarray(cv2.cvtColor(face_np, cv2.COLOR_BGR2RGB))
    tensor = transform(face_pil).unsqueeze(0)  #
    return tensor

def detect_deepfake(model, cropped_faces_dict):
    results = {}

    for idx, face_np in cropped_faces_dict.items():
        if face_np is None:
            # ÏñºÍµ¥Ïù¥ ÏóÜÏúºÎ©¥ ÏòàÏ∏° Î∂àÍ∞Ä -> None Ï≤òÎ¶¨
            results[idx] = None
            continue

        # Ï†ÑÏ≤òÎ¶¨ (preprocess_image Ìï®Ïàò ÏÇ¨Ïö©)
        input_tensor = preprocess_image(face_np)  # (1, C, H, W) ÌÖêÏÑú
# <<<<<<< Ïù¥Î∂ÄÎ∂Ñ?
        input_tensor = input_tensor.to(device)
# =======


        # Î™®Îç∏ Ï∂îÎ°†
        with torch.no_grad():
            output = model(input_tensor)         # shape: (1,1) Í∞ÄÏ†ï
            prob = torch.sigmoid(output).item()  # 0~1 ÌôïÎ•†
            prediction = 1 if prob > 0.7 else 0   # threshold=0.5

        # Í≤∞Í≥º: {Ïù∏Îç±Ïä§: ÏòàÏ∏°Í∞í} ÌòïÌÉúÎ°ú Ï†ÄÏû•
        results[idx] = prediction

    return results
##########################
# real .yaml file
def load_yaml_config(file_path):
    with open(file_path, 'r') as file:
        config = yaml.safe_load(file)
    return config


device = "cuda" if torch.cuda.is_available() else "cpu"
# Call your detection model here
CONFIG_PATH = "../efficient-vit/configs/architecture.yaml"  # Replace with the actual path
config = load_yaml_config(CONFIG_PATH)
# Î™®Îç∏ Î°úÎìú
# MODEL_PATH = "../efficient-vit/pretrained_models/efficient_vit.pth"
MODEL_PATH = "../efficient-vit/pretrained_models/kd_cnn.pth"
model = load_model(MODEL_PATH, device)
###############################

# Page configuration
st.set_page_config(
    page_title="Anttiep's Dashboard",
    page_icon="üèÇ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Sidebar for navigation
with st.sidebar:
    st.title("Page Overview")
    page = st.radio("Go to a page:", ["Home", "Upload Video", "About"])

# 1. Home Page
if page == "Home":
    st.title("üé≠ Team ANTTTIIIEEEPPP's Demo")
    
    # Project Introduction
    st.header("Project ÏÜåÍ∞ú :sunglasses:", divider=True)
    multi = '''
    :rainbow[Îî•ÌéòÏù¥ÌÅ¨ ÌÉêÏßÄ Î™®Îç∏]ÏùÄ Ïù¥ÎØ∏ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Í∞ñÏ∂îÍ≥† ÏûàÏúºÎÇò, Ïó∞ÏÇ∞ ÏûêÏõêÏù¥ Ï†úÌïúÎêú ÌôòÍ≤ΩÏóêÏÑúÎäî ÌôúÏö©Ïù¥ Ï†úÌïúÏ†ÅÏûÖÎãàÎã§.
    Í∑∏Î†áÍ∏∞Ïóê Í≤ΩÎüâÌôîÎêú Îî•ÌéòÏù¥ÌÅ¨ ÌÉêÏßÄ Î™®Îç∏Î°ú Ïù¥Îü¨Ìïú Í∏∞Ïà†Ï†Å Ïû•Î≤ΩÏùÑ ÎÇÆÏ∂∞,
    Îã§ÏñëÌïú Í∏∞ÏóÖ ÏãúÏä§ÌÖú Î∞è Í∏∞Ï°¥ SNS ÌîåÎû´ÌèºÏóê Îî•ÌéòÏù¥ÌÅ¨ ÌÉêÏßÄ Í∏∞Îä•ÏùÑ ÏÜêÏâΩÍ≤å ÌÜµÌï©ÌïòÍ≥†Ïûê Ìï©ÎãàÎã§.
    Îî•ÌéòÏù¥ÌÅ¨ ÌôïÏÇ∞ Î∞©ÏßÄÎ•º ÏúÑÌïú Ïã§ÏãúÍ∞Ñ ÎåÄÏùëÏù¥ Í∞ÄÎä•Ìïú Í≤ΩÎüâÌôî AI Î™®Îç∏ Í∞úÎ∞úÏùÑ Î™©ÌëúÎ°ú ÎëêÍ≥† ÏûàÏäµÎãàÎã§.
    '''
    st.markdown(multi)
    
    # Example Video Section
    st.header("ÌîÑÎ°úÏ†ùÌä∏ Î∞∞Ìè¨ ÏòàÏãú", divider=True)
    VIDEO_URL = "https://www.youtube.com/watch?v=HhMPSuZgJsc"
    st.video(VIDEO_URL)


# 2. Video Upload Page
elif page == "Upload Video":
    st.title("Detect Deepfakes in Videos")
    uploaded_video = st.file_uploader("Upload a Video", type=["mp4"])   # Ï∂îÍ∞Ä Ìï†Í≤É : avi, mov
    
    if uploaded_video is not None:
        st.video(uploaded_video)
        st.markdown("**Analyzing video for deepfakes...**")
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video:
            temp_video.write(uploaded_video.read())
            video_path = temp_video.name
            # output_video_path = temp_video.name ####
        
        # 2FPSÎ°ú ÌîÑÎ†àÏûÑ Ï∂îÏ∂ú

        frames = extract_frames(video_path, fps=2)
        total_frames = len(frames)
        st.write(f"Extracted {total_frames} frames from the video.")
        
        # ÏñºÍµ¥ Í∞êÏßÄ Î∞è ÌÅ¨Î°≠
        cropped_faces = detect_and_crop_faces(frames)
        total_cropped_faces = len(cropped_faces)
        st.write(f"Detected and cropped {total_cropped_faces} faces.")
        # st.write(cropped_faces)
        # # ÌÅ¨Î°≠Îêú ÏñºÍµ¥ Ï∂úÎ†•
        # if cropped_faces:
        #     st.write("Cropped Faces:")
        #     for i, face in enumerate(cropped_faces):
        #         face_image = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
        #         st.image(face_image, caption=f"Cropped Face {i+1}", width=150)  # widthÎ•º 150ÏúºÎ°ú ÏÑ§Ï†ï
        # else:
        #     st.write("No faces detected in the video.")
        # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
        # os.remove(video_path)
        # ÌÅ¨Î°≠Îêú Ïù¥ÎØ∏ÏßÄ ÎîïÏÖîÎÑàÎ¶¨Î°ú ÎßåÎì§Í∏∞
        cropped_faces_dict = {index: value for index, value in enumerate(cropped_faces)}
        #ÎßåÎì† ÎîïÏÖîÌÑ∞Î¶¨ Ï∂úÎ†•
        #st.write(cropped_faces_dict)
        # #
        # # Call your detection model here
        # # Î™®Îç∏ Î°úÎìú
        # MODEL_PATH = "/home/work/Antttiiieeeppp/jh-Lightweight/pretrained_models/effcient_vit.pth"
        # model = load_model(MODEL_PATH)

        # # cropped_faces_dict = {
        # #     0: face_np_0,  # Ïã§Ï†ú ÏñºÍµ¥ Ïù¥ÎØ∏ÏßÄ(np.ndarray)
        # #     1: None,       # ÏñºÍµ¥Ïù¥ ÏóÜÎäî Í≤ΩÏö∞ ÏòàÏãú
        # #     2: face_np_2
        # # }

        # # Îî•ÌéòÏù¥ÌÅ¨ ÌåêÎ≥Ñ
        predictions_dict = detect_deepfake(model, cropped_faces_dict)
        # ÌÖåÏä§Ìä∏
        print(predictions_dict)

        if isinstance(predictions_dict, dict):  # Check if predictions_dict is a valid dictionary
            try:
                # Add badge to the video at specific timestamps
                modified_video = add_badge_with_timestamps(video_path, BADGE_PATH, predictions_dict)
                
                # Display the processed video in the Streamlit app
                st.video(modified_video)
                st.success("Deepfake detected!")
            except Exception as e:
                st.error(f"An error occurred while processing the video: {e}")
        else:
            st.warning("Could not analyze the video. Please try again.")
        # if isinstance(predictions_dict, dict):  # If tis a dictionary -> {1: 0, 2: 1, 3: 1}
        #     # Add subtitles at specific timestamps
        #     # modified_video = adding_subtitles_to_video_w_dict(video_path, predictions_dict) 
        #     modified_video = add_badge_with_timestamps(video_path, BADGE_PATH, output_video_path, predictions_dict)
        #     st.video(modified_video)   
        #     st.success("Deepfake detected!")
        # else:
        #     st.warning("Could not analyze the video. Please try again.")
        # st.success("ÎêúÎã§!")
        
# 3. About Page
elif page == "About":
    st.title("About the Project")
    st.markdown("""
    This project aims to combat the misuse of deepfake technology by providing lightweight AI models 
    capable of real-time detection in resource-limited environments. 
    
    (need to add more here..)
    """)

 # Contributors Section
    st.subheader("Contributors ‚ú®", divider="grey")
    
    # Contributor data
    contributors = [
        {
            "name": "HyeongJun Kim",
            "github": "https://github.com/hoooddy",
            "avatar": "https://avatars.githubusercontent.com/u/35017649?v=4",
        },
        {
            "name": "HyeonWoo Choi",
            "github": "https://github.com/po2955",
            "avatar": "https://avatars.githubusercontent.com/u/84663334?v=4",
        },
        {
            "name": "JaeHee Lee",
            "github": "https://github.com/JaeHeeLE",
            "avatar": "https://avatars.githubusercontent.com/u/153152453?v=4",
        },
        {
            "name": "Seunga Kim",
            "github": "https://github.com/sinya3558",
            "avatar": "https://avatars.githubusercontent.com/u/70243358?v=4",
        }
    ]
    
    # Display contributors
    cols = st.columns(len(contributors))
    for col, contributor in zip(cols, contributors):
        with col:
            st.image(contributor["avatar"], width=100)
            st.markdown(f"**[{contributor['name']}]({contributor['github']})**")
            
# GitHub Repo Section
    st.subheader("GitHub Repository üëæ", divider=True)
    repo_text = '''
    If you want to check our further updates, 
    
    üëâ **Visit our GitHub Repo:** [üíª Antttiiieeeppp](https://github.com/Antttiiieeeppp/Deepfake-Detection-Lightweight)
    '''
    st.markdown(repo_text)
    
# Location Section
    st.subheader("Where we are ü¶ù", divider=True)
    st.caption("Toegye-ro, Jung-go,")
    st.caption("Seoul, Republic of Korea")
    